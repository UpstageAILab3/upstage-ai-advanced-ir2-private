{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://kerneld.tistory.com/32\n","위 문서를 참고해서 ColBERT 콘다 환경을 셋팅하자."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4348,"status":"ok","timestamp":1728593065227,"user":{"displayName":"JJ J","userId":"17499754163730007679"},"user_tz":-540},"id":"xr1Koz1nOnYW","outputId":"b61343c8-11b2-4dab-f05e-f125530a4688"},"outputs":[],"source":["#!pip install openai"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# 학습된 모델을 허깅페이스에 저장/로드 할지 여부.\n","is_use_hf_store_load_trained_model = True\n","\n","if is_use_hf_store_load_trained_model:\n","    hf_repo_id = 'sentence-transformer-klue-temp'\n","    hf_full_repo_id = f\"Kerneld/{hf_repo_id}\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","load_dotenv()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8n8tbsBHmLRV"},"outputs":[],"source":["import json\n","\n","with open(\"../../data/documents.jsonl\") as f:\n","    docs = [json.loads(line) for line in f]\n","\n","# questions_from_contents.jsonl 파일은 Generate_questions_from_content.ipynb 를 실행하면 생성됨.\n","with open(\"questions_from_contents.jsonl\") as f:\n","    qfcs = [json.loads(line) for line in f]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# docs 에서 줄바꿈 문자를 제거 (학습 할때 에러 방지)\n","for doc in docs:\n","    doc['content'] = doc['content'].replace(\"\\n\", \"\")\n","    doc['content'] = doc['content'].replace(\"\\r\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728593068229,"user":{"displayName":"JJ J","userId":"17499754163730007679"},"user_tz":-540},"id":"fGyn-yJsmou6","outputId":"74e40168-6515-45a2-bef0-aaf976b6e76d"},"outputs":[],"source":["print(docs[0])\n","print(qfcs[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUHbWmOCpdqv"},"outputs":[],"source":["docs_only_content = [doc['content'] for doc in docs]\n","docs_only_content[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdD_qUHWp2Ow"},"outputs":[],"source":["qfcs_only_question = [qfc['question'] for qfc in qfcs]\n","qfcs_only_question[0]"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xQg9A-dtp1nB"},"outputs":[],"source":["# 필요한 라이브러리 import\n","\n","import colbert\n","from colbert import Indexer, Searcher\n","from colbert.infra import Run, RunConfig, ColBERTConfig\n","from colbert.data import Queries, Collection"]},{"cell_type":"markdown","metadata":{"id":"RKJdAAbDu7PZ"},"source":["## 1. Indexing\n","\n","빠른 실습을 위해 처음 2000개의 구절만 색인"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vKAdVN5MvDKD"},"outputs":[],"source":["# 기본적인 상수 정의config\n","\n","nbits = 2   # encode each dimension with 2 bits\n","doc_maxlen = 300 # truncate passages at 300 tokens\n","experiment = 'ir_contest'"]},{"cell_type":"markdown","metadata":{"id":"-rHozfXlH9eK"},"source":["한국어 데이터에 대한 색인/검색"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azKR4VULIF3k"},"outputs":[],"source":["#checkpoint = 'colbert-ir/colbertv2.0'\n","checkpoint = 'hunkim/sentence-transformer-klue'\n","#checkpoint = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n","#checkpoint = \"google-bert/bert-base-multilingual-cased\"\n","index_name = 'science_common_sense'\n","\n","with Run().context(RunConfig(nranks = 1, experiment = experiment)):  # nranks specifies the number of GPUs to use\n","    config = ColBERTConfig(doc_maxlen = doc_maxlen, nbits = nbits, kmeans_niters = 4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n","                                                                                # Consider larger numbers for small datasets.\n","    indexer = Indexer(checkpoint = checkpoint, config = config)\n","    indexer.index(name = index_name, collection = docs_only_content, overwrite = True)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def search_docs(query, experiment, index_name, collection, k):\n","    with Run().context(RunConfig(experiment = experiment)):\n","        searcher = Searcher(index = index_name, collection = collection)\n","\n","    # Find the top-3 passages for this query\n","    results = searcher.search(query, k)\n","    \n","    ret = []\n","\n","    # Print out the top-k retrieved passages\n","    for passage_id, passage_rank, passage_score in zip(*results):\n","        print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")\n","        \n","        dic = {\n","            'score': passage_score,\n","            'passage_id': passage_id,\n","            'content': searcher.collection[passage_id],\n","        }\n","        ret.append(dic)\n","    \n","    return ret"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ret = search_docs('나무의 분류에 대해 조사해 보기 위한 방법은?', experiment, index_name, docs_only_content, 3)\n","print(ret)"]},{"cell_type":"markdown","metadata":{"id":"5dUD2BUQSRxk"},"source":["## 3. Training TODO"]},{"cell_type":"markdown","metadata":{"id":"IOch4W2sWRqQ"},"source":["LLM을 활용한 생성데이터로 ColBERT를 학습한 후 더 나은 성능의 모델을 활용하는 방법 실습"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","\n","triples_data = []\n","max_c_idx = len(docs_only_content) - 1\n","\n","for q_idx in range(len(qfcs_only_question)):\n","    # 관련 있는 문서 idx\n","    c_idx = qfcs[q_idx]['docOffset']\n","\n","    # 관련 없는 문서 idx 결정\n","    mc_idx = random.randint(0, max_c_idx)\n","    while mc_idx == c_idx:\n","        mc_idx = random.randint(0, max_c_idx)\n","\n","    triples_data.append(f'{q_idx}, {c_idx}, {mc_idx}')\n","    print(f\"({q_idx}, {c_idx}, {mc_idx}) question: {qfcs_only_question[q_idx]}, relevance doc: {docs_only_content[c_idx]}, not relevance doc: {docs_only_content[mc_idx]}\")\n","\n","print(triples_data)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true},"id":"LO4BG_c9Tpke"},"outputs":[],"source":["# ColBERT 학습을 위하여 학습 데이터를 파일에 저장\n","collection_file = 'collection.tsv'\n","query_file = 'query.tsv'\n","triples_file = 'triples'\n","\n","with open(collection_file, 'w') as f:\n","  for i,item in enumerate(docs_only_content):\n","    f.write(f'{i}\\t{item}\\n')\n","\n","with open(query_file, 'w') as f:\n","  for i,item in enumerate(qfcs_only_question):\n","    f.write(f'{i}\\t{item}\\n')\n","\n","with open(triples_file, 'w') as f:\n","  for i,item in enumerate(triples_data):\n","    f.write(f'[{item}]\\n')"]},{"cell_type":"markdown","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"rQneCEKgUYqQ","outputId":"70b7e9e6-2b00-4f0e-b9d8-2a9e54d1a1da"},"source":["### 3.2 새로 만든 데이터로 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0L_N_CNLzpi7"},"outputs":[],"source":["from colbert.infra import Run, RunConfig, ColBERTConfig\n","from colbert import Trainer\n","\n","if __name__=='__main__':\n","    with Run().context(RunConfig(nranks=1, experiment=\"training\")):\n","\n","        config = ColBERTConfig(\n","            bsize=24,\n","            root=\"./experiments\",\n","        )\n","\n","        trainer = Trainer(\n","            triples=triples_file,\n","            queries=query_file,\n","            collection=collection_file,\n","            config=config,\n","        )\n","\n","        # Pretrained model을 한국어 기반 모델로 설정해 준다.\n","        checkpoint_path = trainer.train(checkpoint='hunkim/sentence-transformer-klue')\n","        #checkpoint_path = trainer.train(checkpoint=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n","        #checkpoint_path = trainer.train(checkpoint=\"google-bert/bert-base-multilingual-cased\")\n","        #checkpoint_path = trainer.train()\n","\n","        print(f\"Saved checkpoint to {checkpoint_path}...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0L_N_CNLzpi7"},"outputs":[],"source":["# # 학습된 모델의 위치\n","!find experiments/training -name colbert"]},{"cell_type":"markdown","metadata":{},"source":["학습된 모델을 허깅페이스에 업로드"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","from huggingface_hub import HfApi\n","\n","if is_use_hf_store_load_trained_model:\n","    model_save_path = 'experiments/training/none/2024-10/16/05.29.48/checkpoints/colbert'\n","    \n","    login(token=os.getenv('HF_TOKEN'))\n","    \n","    api = HfApi()\n","    api.create_repo(repo_id=hf_repo_id)\n","\n","    api.upload_folder(\n","        folder_path=model_save_path,\n","        repo_id=hf_full_repo_id,\n","        repo_type=\"model\",\n","    )"]},{"cell_type":"markdown","metadata":{"id":"jYKObQSYYBdF"},"source":["학습된 모델로 다시 색인 및 검색"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xvgwsba3YL4b"},"outputs":[],"source":["# 위에서 확인한 학습된 모델의 위치를 checkpoint에 넣어줌\n","if is_use_hf_store_load_trained_model:\n","    checkpoint = hf_full_repo_id\n","else:\n","    checkpoint = 'experiments/training/none/2024-10/16/12.01.30/checkpoints/colbert'\n","    \n","experiment = 'after_trained'\n","index_name = 'science_common_sense'\n","\n","with Run().context(RunConfig(nranks=1, experiment=experiment)):  # nranks specifies the number of GPUs to use\n","    config = ColBERTConfig(nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n","                                                                                # Consider larger numbers for small datasets.\n","    indexer = Indexer(checkpoint = checkpoint, config = config)\n","    indexer.index(name = index_name, collection = docs_only_content, overwrite=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xvgwsba3YL4b"},"outputs":[],"source":["ret = search_docs('나무의 분류에 대해 조사해 보기 위한 방법은?', experiment, index_name, docs_only_content, 3)\n","print(ret)"]},{"cell_type":"markdown","metadata":{},"source":["학습된 모델로 추론을 해보자~"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["from openai import OpenAI\n","import traceback\n","\n","# OpenAI API 키를 환경변수에 설정\n","os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n","#os.environ[\"OPENAI_API_KEY\"] = \"Your API Key\"\n","\n","client = OpenAI()\n","# 사용할 모델을 설정(여기서는 gpt-3.5-turbo-1106 모델 사용)\n","llm_model = \"gpt-3.5-turbo-1106\""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# RAG 구현에 필요한 Question Answering을 위한 LLM  프롬프트\n","persona_qa = \"\"\"\n","## Role: 과학 상식 전문가\n","\n","## Instructions\n","- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n","- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n","- 한국어로 답변을 생성한다.\n","\"\"\"\n","\n","# RAG 구현에 필요한 질의 분석 및 검색 이외의 일반 질의 대응을 위한 LLM 프롬프트\n","persona_function_calling = \"\"\"\n","## Role: 과학 상식 전문가\n","\n","## Instruction\n","- 사용자가 대화를 통해 과학 지식에 관한 주제로 질문하면 search api를 호출할 수 있어야 한다.\n","- search api를 호출할 때, 사용자의 대화가 멀티턴이 아니라면, 사용자의 대화 내용을 그대로 standalone_query 에 대입한다.\n","- 과학 상식과 관련되지 않은 나머지 대화 메시지에는 적절한 대답을 생성한다.\n","\"\"\"\n","\n","# Function calling에 사용할 함수 정의\n","tools = [\n","    {\n","        \"type\": \"function\",\n","        \"function\": {\n","            \"name\": \"search\",\n","            \"description\": \"search relevant documents\",\n","            \"parameters\": {\n","                \"properties\": {\n","                    \"standalone_query\": {\n","                        \"type\": \"string\",\n","                        \"description\": \"Final query suitable for use in search from the user messages history.\"\n","                    }\n","                },\n","                \"required\": [\"standalone_query\"],\n","                \"type\": \"object\"\n","            }\n","        }\n","    },\n","]\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# LLM과 검색엔진을 활용한 RAG 구현\n","def answer_question(messages):\n","    # 함수 출력 초기화\n","    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n","\n","    # 질의 분석 및 검색 이외의 질의 대응을 위한 LLM 활용\n","    msg = [{\"role\": \"system\", \"content\": persona_function_calling}] + messages\n","    try:\n","        result = client.chat.completions.create(\n","            model=llm_model,\n","            messages=msg,\n","            tools=tools,\n","            #tool_choice={\"type\": \"function\", \"function\": {\"name\": \"search\"}},\n","            temperature=0,\n","            seed=1,\n","            timeout=10\n","        )\n","    except Exception as e:\n","        traceback.print_exc()\n","        return response\n","\n","    # 검색이 필요한 경우 검색 호출후 결과를 활용하여 답변 생성\n","    if result.choices[0].message.tool_calls:\n","        tool_call = result.choices[0].message.tool_calls[0]\n","        function_args = json.loads(tool_call.function.arguments)\n","        standalone_query = function_args.get(\"standalone_query\")\n","        response[\"standalone_query\"] = standalone_query\n","\n","        hits_merged_list = search_docs(standalone_query, experiment, index_name, docs_only_content, 3)\n","        \n","        retrieved_context = []\n","        for i, rst in enumerate(hits_merged_list):\n","            if i >= 3:\n","                break   # 최대 3개까지만..\n","            \n","            doc = docs[rst['passage_id']]\n","            \n","            retrieved_context.append(doc[\"content\"])\n","            \n","            response[\"topk\"].append(doc[\"docid\"])\n","            response[\"references\"].append({\"score\": rst[\"score\"], \"content\": doc[\"content\"]})\n","\n","        content = json.dumps(retrieved_context)\n","        messages.append({\"role\": \"assistant\", \"content\": content})\n","        msg = [{\"role\": \"system\", \"content\": persona_qa}] + messages\n","        try:\n","            qaresult = client.chat.completions.create(\n","                    model=llm_model,\n","                    messages=msg,\n","                    temperature=0,\n","                    seed=1,\n","                    timeout=30\n","                )\n","        except Exception as e:\n","            traceback.print_exc()\n","            return response\n","        response[\"answer\"] = qaresult.choices[0].message.content\n","\n","    # 검색이 필요하지 않은 경우 바로 답변 생성\n","    else:\n","        response[\"answer\"] = result.choices[0].message.content\n","\n","    return response"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# 평가를 위한 파일을 읽어서 각 평가 데이터에 대해서 결과 추출후 파일에 저장\n","def eval_rag(eval_filename, output_filename):\n","    with open(eval_filename) as f, open(output_filename, \"w\") as of:\n","        idx = 0\n","        for line in f:\n","            # if idx > 0:\n","            #     break\n","            j = json.loads(line)\n","            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n","            response = answer_question(j[\"msg\"])\n","            print(f'Answer: {response[\"answer\"]}\\n')\n","\n","            # 대회 score 계산은 topk 정보를 사용, answer 정보는 LLM을 통한 자동평가시 활용\n","            output = {\"eval_id\": j[\"eval_id\"], \"standalone_query\": response[\"standalone_query\"], \"topk\": response[\"topk\"], \"answer\": response[\"answer\"], \"references\": response[\"references\"]}\n","            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n","            idx += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 평가 데이터에 대해서 결과 생성 - 파일 포맷은 jsonl이지만 파일명은 csv 사용\n","submission_filename = \"submission_016-4.csv\"\n","\n","eval_rag(\"../../data/eval.jsonl\", submission_filename)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb","timestamp":1704182152607}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.20"}},"nbformat":4,"nbformat_minor":0}

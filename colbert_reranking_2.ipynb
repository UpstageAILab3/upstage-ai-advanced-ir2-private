{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR_DCxZUH3bs"
      },
      "source": [
        "# RAG - ColBERT IR, gpt tubo 3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pipl3WrJIpyD"
      },
      "source": [
        "## IR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knoTZCmgH_3z"
      },
      "source": [
        "### 라이브러리 설치 및 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mGa65b7yf_RM"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 import\n",
        "\n",
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection\n",
        "import colbert\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"/data/ephemeral/home/upstage-ai-advanced-ir2/data/documents.jsonl\") as f:\n",
        "    docs = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'docid': '42508ee0-c543-4338-878e-d98c6babee66', 'src': 'ko_mmlu__nutrition__test', 'content': '건강한 사람이 에너지 균형을 평형 상태로 유지하는 것은 중요합니다. 에너지 균형은 에너지 섭취와 에너지 소비의 수학적 동등성을 의미합니다. 일반적으로 건강한 사람은 1-2주의 기간 동안 에너지 균형을 달성합니다. 이 기간 동안에는 올바른 식단과 적절한 운동을 통해 에너지 섭취와 에너지 소비를 조절해야 합니다. 식단은 영양가 있는 식품을 포함하고, 적절한 칼로리를 섭취해야 합니다. 또한, 운동은 에너지 소비를 촉진시키고 근육을 강화시킵니다. 이렇게 에너지 균형을 유지하면 건강을 유지하고 비만이나 영양 실조와 같은 문제를 예방할 수 있습니다. 따라서 건강한 사람은 에너지 균형을 평형 상태로 유지하는 것이 중요하며, 이를 위해 1-2주의 기간 동안 식단과 운동을 조절해야 합니다.'}\n",
            "건강한 사람이 에너지 균형을 평형 상태로 유지하는 것은 중요합니다. 에너지 균형은 에너지 섭취와 에너지 소비의 수학적 동등성을 의미합니다. 일반적으로 건강한 사람은 1-2주의 기간 동안 에너지 균형을 달성합니다. 이 기간 동안에는 올바른 식단과 적절한 운동을 통해 에너지 섭취와 에너지 소비를 조절해야 합니다. 식단은 영양가 있는 식품을 포함하고, 적절한 칼로리를 섭취해야 합니다. 또한, 운동은 에너지 소비를 촉진시키고 근육을 강화시킵니다. 이렇게 에너지 균형을 유지하면 건강을 유지하고 비만이나 영양 실조와 같은 문제를 예방할 수 있습니다. 따라서 건강한 사람은 에너지 균형을 평형 상태로 유지하는 것이 중요하며, 이를 위해 1-2주의 기간 동안 식단과 운동을 조절해야 합니다.\n",
            "42508ee0-c543-4338-878e-d98c6babee66\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'ko_mmlu__nutrition__test'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# docs 에서 줄바꿈 문자를 제거 (학습 할때 에러 방지)\n",
        "for doc in docs:\n",
        "    doc['content'] = doc['content'].replace(\"\\n\", \"\")\n",
        "    doc['content'] = doc['content'].replace(\"\\r\", \"\")\n",
        "print(docs[0])\n",
        "collection = [doc['content'] for doc in docs]\n",
        "print(collection[0])\n",
        "docid_list = [doc['docid'] for doc in docs]\n",
        "print(docid_list[0])\n",
        "src_list = [doc['src'] for doc in docs]\n",
        "src_list[0]    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epRb1LDTIGzq"
      },
      "source": [
        "### ColBER 색인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvv9hfLdtBkg",
        "outputId": "0f4d1489-b1d6-4144-9681-ca7f1007a135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA GeForce RTX 3090\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # True여야 합니다.\n",
        "print(torch.cuda.get_device_name(0))  # 'Tesla T4'와 같은 GPU 이름이 나와야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p44q7qytNaVO",
        "outputId": "a7702a05-4c36-4489-bb49-3cd0e2baf3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # True가 출력되면 GPU 사용이 가능함을 의미합니다.\n",
        "print(torch.cuda.device_count())  # 사용 가능한 GPU의 수를 출력합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 위에서 확인한 학습된 모델의 위치를 checkpoint에 넣어줌\n",
        "checkpoint = 'experiments/sample_ko_new_2/none/2024-10/16/07.20.10/checkpoints/colbert'\n",
        "experiment = 'sample_ko_new_2'\n",
        "index_name = 'science_common_sense_2'\n",
        "nbits = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "[Oct 16, 15:15:53] #> Note: Output directory /data/ephemeral/home/upstage-ai-advanced-ir2/experiments/sample_ko_new/indexes/science_common_sense already exists\n",
            "\n",
            "\n",
            "[Oct 16, 15:15:53] #> Will delete 10 files already at /data/ephemeral/home/upstage-ai-advanced-ir2/experiments/sample_ko_new/indexes/science_common_sense in 20 seconds...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment=experiment)):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "                                                                                # Consider larger numbers for small datasets.\n",
        "    indexer = Indexer(checkpoint = checkpoint, config = config)\n",
        "    indexer.index(name = index_name, collection = collection, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPgheHQdh7tb",
        "outputId": "c21896a5-a4b2-48a8-9afe-87181ec1cd06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Oct 17, 01:32:55] #> Loading codec...\n",
            "[Oct 17, 01:32:55] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Oct 17, 01:32:55] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Oct 17, 01:32:56] #> Loading IVF...\n",
            "[Oct 17, 01:32:56] #> Loading doclens...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1036.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Oct 17, 01:32:56] #> Loading codes and residuals...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 37.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# 검색을 수행하기 위한 Searcher 객체를 생성합니다.\n",
        "# ColBERT 인덱스를 기반으로 검색 작업을 수행합니다.\n",
        "\n",
        "with Run().context(RunConfig(experiment=experiment)):  # 실행 환경을 설정합니다. 여기서는 'notebook'이라는 이름으로 실험을 정의합니다.\n",
        "    searcher = Searcher(index=index_name, collection=collection)  # 지정된 인덱스와 컬렉션을 사용하여 Searcher 객체를 초기화합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MISgcL4tILtL"
      },
      "source": [
        "### Colbert 색인 문서 검색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 문서 검색 test\n",
        "# queries = [\"나무 분류하는 방법은?\"]  # 검색할 질의 목록입니다.\n",
        "\n",
        "# # 각 질의에 대해 검색을 수행합니다.\n",
        "# for query in queries:\n",
        "#     print(f\"#> {query}\")  # 현재 검색 질의를 출력합니다.\n",
        "\n",
        "#     # 검색 질의에 대한 상위 5개의 문서를 검색합니다.\n",
        "#     results = searcher.search(query, k=3)  # k는 검색 결과에서 반환할 상위 문서의 개수를 의미합니다.\n",
        "\n",
        "#     # 검색된 상위 k개의 문서를 출력합니다.\n",
        "#     for passage_id, passage_rank, passage_score in zip(*results):\n",
        "#         # 검색 결과의 순위, 점수 및 해당 문서를 출력합니다.\n",
        "#         print(f\"{docid_list[passage_id]}\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_OXai3tki7O9"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import traceback\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "\n",
        "load_dotenv(dotenv_path='/data/ephemeral/home/upstage-ai-advanced-ir2/OPENAI_API_KEY.env')\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI()  \n",
        "llm_model = \"gpt-3.5-turbo-1106\" #\"gpt-4-turbo-2024-04-09\" #\"gpt-3.5-turbo-1106\" #\"gpt-4-turbo-2024-04-09\"\n",
        "\n",
        "persona_qa = \"\"\"\n",
        "## Role: 지식 백과사전\n",
        "\n",
        "## Instructions\n",
        "- 사용자의 이전 메시지 정보 및 주어진 Reference 정보를 활용하여 간결하게 답변을 생성한다.\n",
        "- 주어진 검색 결과 정보로 대답할 수 없는 경우는 정보가 부족해서 답을 할 수 없다고 대답한다.\n",
        "- 사용자가 사용한 언어로 답변을 생성한다.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 질의 쿼리는 여기에 추가(종결어미)\n",
        "persona_function_calling = \"\"\"\n",
        "## Role: 과학 상식 전문가\n",
        "\n",
        "## Instruction\n",
        "- 사용자가 대화를 통해 지식에 관한 주제로 질문하면 search api를 호출할 수 있어야 한다.\n",
        "- 지식과 관련되지 않은 나머지 대화 메시지에는 적절한 대답을 생성한다.\n",
        "\"\"\"\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",  \n",
        "        \"function\": {\n",
        "            \"name\": \"search\",  \n",
        "            \"description\": \"search relevant documents\",  \n",
        "            \"parameters\": {\n",
        "                \"properties\": {  \n",
        "                    \"standalone_query\": {\n",
        "                        \"type\": \"string\",  \n",
        "                        \"description\": \"사용자 메세지 기록으로부터 검색에 사용할 적합한 최종 질의 생성\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"standalone_query\"],  \n",
        "                \"type\": \"object\" \n",
        "            }\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "def crossencoder_rerank(query, topk_results):\n",
        "    cross_encoder_inputs = [(query, searcher.collection[passage_id]) for passage_id, _, _ in zip(*topk_results)]\n",
        "    \n",
        "    # Cross Encoder로 재랭킹\n",
        "    rerank_scores = cross_encoder_model.predict(cross_encoder_inputs)\n",
        "    \n",
        "    # 점수를 float으로 변환\n",
        "    rerank_scores = [float(score) for score in rerank_scores]\n",
        "\n",
        "    # ColBERT 결과에서 docid와 점수를 함께 반환\n",
        "    reranked_topk = sorted(zip(topk_results[0], rerank_scores), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # 상위 3개의 문서의 docid만 추출\n",
        "    return [docid_list[passage_id] for passage_id, _ in reranked_topk[:3]]\n",
        "\n",
        "\n",
        "def answer_question(messages):\n",
        "    print(\"\\n=== Answer Question ===\")\n",
        "    print(\"Messages: \", messages)\n",
        "\n",
        "    response = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"\"}\n",
        "\n",
        "    msg = [{\"role\": \"system\", \"content\": persona_function_calling}] + messages\n",
        "    try:\n",
        "        result = client.chat.completions.create(\n",
        "            model=llm_model,\n",
        "            messages=msg,\n",
        "            tools=tools,\n",
        "            temperature=0,\n",
        "            seed=1,\n",
        "            timeout=10\n",
        "        )\n",
        "        print(\"LLM Response: \", result)\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        return response\n",
        "\n",
        "    response_message = result.choices[0].message\n",
        "\n",
        "    if response_message.tool_calls:\n",
        "        for tool_call in response_message.tool_calls:\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            standalone_query = function_args.get(\"standalone_query\")\n",
        "            #print(\"Standalone Query: \", standalone_query)\n",
        "\n",
        "            # ColBERT 검색 실행\n",
        "            results = searcher.search(standalone_query, k=20)\n",
        "            #print(\"ColBERT Search Results: \", results)\n",
        "            #print(\"searcher : \", searcher)\n",
        "            response[\"standalone_query\"] = standalone_query\n",
        "            retrieved_context = []\n",
        "\n",
        "            # ColBERT 검색 결과에서 문서 ID와 점수 저장\n",
        "            for passage_id, passage_rank, passage_score in zip(*results):\n",
        "                retrieved_context.append(searcher.collection[passage_id])\n",
        "                response[\"references\"].append({\"score\": float(passage_score), \"content\": searcher.collection[passage_id]})\n",
        "            #print(\"Retrieved Context: \", retrieved_context)\n",
        "\n",
        "            # Cross-Encoder로 재랭킹\n",
        "            topk = crossencoder_rerank(standalone_query, results)\n",
        "            response[\"topk\"] = topk\n",
        "\n",
        "            # 도구 호출 결과로 LLM이 답변을 생성하도록 다시 요청\n",
        "            final_prompt = [{\"role\": \"user\", \"content\": f\"이 문서를 기반으로 요약한 답변을 생성해줘: {retrieved_context}\"}]\n",
        "            try:\n",
        "                final_response = client.chat.completions.create(\n",
        "                    model=llm_model,\n",
        "                    messages=final_prompt,\n",
        "                    temperature=0,\n",
        "                    seed=1,\n",
        "                    timeout=20\n",
        "                )\n",
        "                response[\"answer\"] = final_response.choices[0].message.content\n",
        "            except Exception as e:\n",
        "                traceback.print_exc()\n",
        "    else:\n",
        "        response[\"answer\"] = response_message.content if response_message.content else \"\"\n",
        "\n",
        "    #print(\"Response: \", response)\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "# eval_rag 함수: question 데이터를 파일에서 읽어와 처리한 후 결과를 output에 저장\n",
        "def eval_rag(eval_filename, output_filename):\n",
        "    with open(eval_filename, 'r', encoding='utf-8') as f, open(output_filename, 'w', encoding='utf-8') as of:\n",
        "        idx = 0\n",
        "        for line in f:\n",
        "            j = json.loads(line)\n",
        "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
        "\n",
        "            # 메시지를 처리하여 검색 결과 및 답변 생성\n",
        "            messages = [{\"role\": \"user\", \"content\": msg[\"content\"]} for msg in j[\"msg\"]]\n",
        "            response = answer_question(messages)\n",
        "\n",
        "            # 결과를 출력 파일에 저장\n",
        "            output = {\n",
        "                \"eval_id\": j[\"eval_id\"],\n",
        "                \"standalone_query\": response[\"standalone_query\"],\n",
        "                \"topk\": response[\"topk\"],\n",
        "                \"answer\": response[\"answer\"],\n",
        "                \"references\": response[\"references\"][:3]\n",
        "            }\n",
        "            of.write(f'{json.dumps(output, ensure_ascii=False)}\\n')\n",
        "            idx += 1\n",
        "\n",
        "# Cross-encoder model loading\n",
        "from sentence_transformers import CrossEncoder\n",
        "cross_encoder_model = CrossEncoder('output/klue-cross-encoder-v1_13q', max_length=512)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 최종 실행 예시\n",
        "eval_rag('/data/ephemeral/home/upstage-ai-advanced-ir2/data/eval_copy.jsonl',\n",
        "         '/data/ephemeral/home/upstage-ai-advanced-ir2/submission_test/prompt_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

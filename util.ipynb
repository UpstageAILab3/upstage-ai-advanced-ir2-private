{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP 점수 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 두 파일에서 topk 항목을 불러옴\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Average Precision (AP) 계산\n",
    "def calculate_ap(true_topk, predicted_topk):\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, pred in enumerate(predicted_topk):\n",
    "        if pred in true_topk:\n",
    "            hits += 1\n",
    "            score += hits / (i + 1)\n",
    "    if hits == 0:\n",
    "        return 0.0\n",
    "    return score / len(true_topk)\n",
    "\n",
    "def calculate_map(ground_truth_file, prediction_file, max_k=3):\n",
    "    ground_truth_data = load_jsonl(ground_truth_file)\n",
    "    prediction_data = load_jsonl(prediction_file)\n",
    "\n",
    "    # 각 k별로 MAP을 저장할 딕셔너리\n",
    "    map_scores_by_k = {k: [] for k in range(1, max_k + 1)}\n",
    "\n",
    "    for gt_item in ground_truth_data:\n",
    "        pred_item = next((item for item in prediction_data if item['eval_id'] == gt_item['eval_id']), None)\n",
    "        if pred_item:\n",
    "            for k in range(1, max_k + 1):\n",
    "                top_gt = gt_item['topk'][:k]\n",
    "                top_pred = pred_item['topk'][:k]\n",
    "                ap = calculate_ap(top_gt, top_pred)  # 상위 k개의 항목을 사용하여 AP 계산\n",
    "                map_scores_by_k[k].append(ap)\n",
    "\n",
    "    # 각 k별로 MAP 계산\n",
    "    map_scores = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        map_score = sum(map_scores_by_k[k]) / len(map_scores_by_k[k]) if map_scores_by_k[k] else 0.0\n",
    "        map_scores.append(f\"top-{k}: {map_score:.4f}\")\n",
    "\n",
    "    # 한 번에 모든 top-k 결과를 출력\n",
    "    result = \" | \".join(map_scores)\n",
    "    print(f\"{prediction_file} \" + result)\n",
    "\n",
    "    return map_scores_by_k\n",
    "\n",
    "ground_truth_file = './output/0.8614.csv'\n",
    "print(f\"ground_truth_file : {ground_truth_file}\")\n",
    "map_score = calculate_map(ground_truth_file, './output/1015-2242_topk3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval.jsonl에 다른 파일의 standalone_query 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 파일 경로 설정\n",
    "a_file = './data/eval.jsonl'  # eval_id 필드가 있는 첫 번째 파일\n",
    "b_file = './output/0.8409.csv'  # query 필드가 있는 두 번째 파일\n",
    "output_file = './data/eval_8409.jsonl'  # 결과를 저장할 파일\n",
    "\n",
    "# b.jsonl에서 eval_id와 standalone_query의 매핑을 생성\n",
    "b_mapping = {}\n",
    "with open(b_file, 'r', encoding='utf-8') as b_infile:\n",
    "    for line in b_infile:\n",
    "        b_doc = json.loads(line)\n",
    "        # eval_id를 키로, standalone_query를 값으로 추가\n",
    "        b_mapping[b_doc['eval_id']] = b_doc['standalone_query']\n",
    "\n",
    "# a.jsonl 파일에서 eval_id에 따른 standalone_query 추가\n",
    "with open(a_file, 'r', encoding='utf-8') as a_infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in a_infile:\n",
    "        a_doc = json.loads(line)\n",
    "        eval_id = a_doc['eval_id']\n",
    "        \n",
    "        # b.jsonl에서 해당 eval_id에 대한 standalone_query 가져오기\n",
    "        standalone_query = b_mapping.get(eval_id, None)  # 존재하지 않을 경우 None\n",
    "        \n",
    "        # 새로운 문서 생성\n",
    "        new_doc = {\n",
    "            'eval_id': eval_id,\n",
    "            'standalone_query': standalone_query,\n",
    "            'msg': a_doc['msg']  # 기존 msg 필드 추가\n",
    "        }\n",
    "        \n",
    "        # JSONL 형식으로 저장\n",
    "        outfile.write(json.dumps(new_doc, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하드보팅\n",
    "1. submission 파일 생성\n",
    "2. 1번째 topk가 같은 아이템 목록 파일 생성\n",
    "3. 1번째 topk가 다른 아이템 목록 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Union\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "def process_jsonl_files(input_paths: Union[List[str], str]):\n",
    "    current_time = datetime.fromtimestamp(time.time(), tz=ZoneInfo(\"Asia/Seoul\")).strftime(\"%m%d-%H%M\")\n",
    "\n",
    "    input_files = []\n",
    "    \n",
    "    if isinstance(input_paths, str):\n",
    "        if os.path.isdir(input_paths):\n",
    "            print(f\"Processing directory: {input_paths}\")\n",
    "            input_files = [os.path.join(input_paths, f) for f in os.listdir(input_paths) if f.endswith('.jsonl')]\n",
    "        else:\n",
    "            raise ValueError(\"If a single path is provided, it must be a directory.\")\n",
    "    else:\n",
    "        input_files = input_paths\n",
    "\n",
    "    print(f\"Found {len(input_files)} JSONL files to process.\")\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No JSONL files found to process.\")\n",
    "        return\n",
    "\n",
    "    eval_id_data = defaultdict(list)\n",
    "\n",
    "    for file_path in input_files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    eval_id = item['eval_id']\n",
    "                    topk = item.get('topk', [])\n",
    "                    standalone_query = item.get('standalone_query', '')\n",
    "                    \n",
    "                    eval_id_data[eval_id].append({\n",
    "                        'file': os.path.basename(file_path),\n",
    "                        'topk': topk,\n",
    "                        'standalone_query': standalone_query\n",
    "                    })\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON in file {file_path}. Skipping this line.\")\n",
    "                except KeyError:\n",
    "                    print(f\"Missing 'eval_id' in a line in file {file_path}. Skipping this line.\")\n",
    "\n",
    "    same_items = []\n",
    "    diff_items = []\n",
    "    voted_items = []\n",
    "\n",
    "    for eval_id, items in eval_id_data.items():\n",
    "        if len(items) > 1:\n",
    "            first_topk = items[0]['topk'][0] if items[0]['topk'] else None\n",
    "            \n",
    "            # Find the first non-empty standalone_query\n",
    "            standalone_query = next((item['standalone_query'] for item in items if item['standalone_query']), '')\n",
    "            \n",
    "            if all(item['topk'] and item['topk'][0] == first_topk for item in items):\n",
    "                same_items.append({\n",
    "                    'eval_id': eval_id,\n",
    "                    'representative_query': standalone_query,\n",
    "                    'topk': items[0]['topk'],\n",
    "                    'files': [item['file'] for item in items]\n",
    "                })\n",
    "            else:\n",
    "                diff_items.append({\n",
    "                    'eval_id': eval_id,\n",
    "                    'representative_query': standalone_query,\n",
    "                    'diff': items,\n",
    "                })\n",
    "\n",
    "            # Hard voting for top 3\n",
    "            all_topk = [item for sublist in [item['topk'] for item in items] for item in sublist]\n",
    "            vote_count = Counter(all_topk)\n",
    "            top_3 = [item for item, _ in vote_count.most_common(3)]\n",
    "            \n",
    "            voted_items.append({\n",
    "                'eval_id': eval_id,\n",
    "                'representative_query': standalone_query,\n",
    "                'topk': top_3,\n",
    "                'original_items': items\n",
    "            })\n",
    "\n",
    "    # Write same items to a file\n",
    "    file_name = f\"./voting/{current_time}_same_1st_items.jsonl\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        for item in same_items:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(f\"Wrote {len(same_items)} items to {file_name}\")\n",
    "\n",
    "    # Write different items to a file\n",
    "    file_name = f\"./voting/{current_time}_diff_1st_items.jsonl\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        for item in diff_items:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(f\"Wrote {len(diff_items)} items to {file_name}\")\n",
    "\n",
    "    # Write voted items to a file\n",
    "    file_name = f\"./voting/{current_time}_voted_items.csv\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        for item in voted_items:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    print(f\"Wrote {len(voted_items)} items to {file_name}\")\n",
    "\n",
    "    print(f\"\\nProcessing complete. {len(same_items)} items with same top-k value and {len(diff_items)} items with different top-k values.\")\n",
    "    print(f\"{len(voted_items)} items processed for hard voting.\")\n",
    "    print(\"\\nFirst few voted items:\")\n",
    "    for item in voted_items[:3]:  # Print only first 3 items for brevity\n",
    "        print(json.dumps(item, indent=2, ensure_ascii=False))\n",
    "\n",
    "input_files = ['./submissions/0.9023-0.9061.csv', './submissions/0.8962-0.8970.csv', './submissions/0.8689-0.8712.csv']\n",
    "process_jsonl_files(input_files)\n",
    "\n",
    "# foler_name = './output_high'\n",
    "# process_jsonl_files(foler_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
